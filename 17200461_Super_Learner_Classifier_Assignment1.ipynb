{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: The Super Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Add more packages as required\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from random import randint\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import metrics\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Super Learner Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Super Learner* is a heterogeneous stacked ensemble classifier. This is a classification model that uses a set of base classifiers of different types, the outputs of which are then combined in another classifier at the stacked layer. The Super Learner was described in [(van der Laan et al, 2007)](https://pdfs.semanticscholar.org/19e9/c732082706f39d2ba12845851309714db135.pdf) but the stacked ensemble idea has been around for a long time. \n",
    "\n",
    "Figure 1 shows a flow diagram of the Super Learner process (this is from (van der Laan et al, 2007) and the process is also described in the COMP47590 lecture \"[COMP47590 2017-2018 L04 Supervised Learning Ensembles 3](https://www.dropbox.com/s/1ksx94nxtuyn4l8/COMP47590%202017-2018%20L04%20Supervised%20Learning%20Ensembles%203.pdf?raw=1)\"). The base classifiers are trained and their outputs are combined along with the training dataset labels into a training set for the stack layer classifier. To avoid overfitting the generation of the stacked layer training set uses a k-fold cross validation process (described as V-fold in Figure 1). To further add variety to the base estimators a bootstrapping selection (as is used in the bagging ensemble approach).\n",
    " \n",
    "![Super Learner Process Flow](SuperLearnerProcessFlow.png \"Logo Title Text 1\")\n",
    "Figure 1: A flow diagram for the Super Learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SuperLearnerClassifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer and a \n",
    "    aggregation model at the aggregation layer. A k-fold cross validation is used\n",
    "    to gnerate training data for the stack layer model.\n",
    "\n",
    "    \n",
    "    Attributes\n",
    "    \n",
    "    _init_ : This attribute is the constructor for the classifier objects which initializes them with default values.\n",
    "    \n",
    "    fit: This attribute requires X i.e the Data and y i.e the Labels as an input to perform training of Super Learner Classifier\n",
    "    \n",
    "    predict: Super Learner predict method predict the labels and return them.\n",
    "    \n",
    "    predict_proba: Super Learner predict_proba method predicts the probability based labels.\n",
    "    \n",
    "    getBaseClassifierPerformance: To find the diversity and predictive power of the base estimators we are using for the \n",
    "                                  Super Learner Classifier\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self,modelslists=None,aggregator=None,Vfold=None,probability_flag=False,original_features=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Setup a SuperLearner classifier .\n",
    "        \n",
    "        Parameters:\n",
    "        -modelslists: Here the superlearner algorithm will contain some paramters like the list of base estimators \n",
    "        (heterogenous models).\n",
    "        \n",
    "        -aggregator: This parameter specifies which model is used at stacked layer.\n",
    "        \n",
    "        -probability flag: To decide whether the output provided by the base estimators will be label based or probability based and these outputs will be given as an input to stack layer.\n",
    "        \n",
    "        -V-fold: The folds on which the data of base estimators will be splitted i.e the cross validation will happen.\n",
    "        \n",
    "        -original_features: This flag specifies if we need to add original features to the Stack layer.\n",
    "\n",
    "        \"\"\"  \n",
    "        #Initializing the parameters with their default values\n",
    "        self.probability_flag=probability_flag\n",
    "        self.modelslists=modelslists\n",
    "        self.aggregator=aggregator\n",
    "        self.original_features=original_features  \n",
    "        self.Vfolds=Vfold \n",
    "    \n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Return\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        s\n",
    "        -------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"       \n",
    "   \n",
    "    #TASK 1: To implement the basic charecteristics of Super Learner Classifier (Heterogenous Base estimators with some default parameters setting.)\n",
    "    \n",
    "    #Define all the default models internally, if the user doesn't know what all models should be used.\n",
    "        if self.modelslists==None:\n",
    "            DecTreeClf= tree.DecisionTreeClassifier()\n",
    "            SVMClf= SVC(probability=True)\n",
    "            KnnClf=KNeighborsClassifier(5)\n",
    "            SGDClf=linear_model.SGDClassifier()\n",
    "            LogClf=linear_model.LogisticRegression()\n",
    "            NBClf=naive_bayes.GaussianNB()\n",
    "            RFClf=RandomForestClassifier()\n",
    "            self.modelslists=[DecTreeClf,RFClf,KnnClf,SVMClf,LogClf,NBClf]\n",
    "       \n",
    "    #Check the Vfolds value require to perform vfolds cross validations on base estimators\n",
    "        if self.Vfolds==None:\n",
    "            kfold=KFold(n_splits=5)\n",
    "        else:\n",
    "            kfold=KFold(n_splits=Vfold)\n",
    "     \n",
    "    #Declare the DataFrames that will be used in the method       \n",
    "        pred_array_final=pd.DataFrame() #To store the final Stack Layer Data Frame\n",
    "        pred_label_final=pd.DataFrame() #To store the final Stack Layer Target Labels\n",
    "        pred_array=pd.DataFrame()       #To store the data from the predicted labels from the base estimators\n",
    "        proba_dataframe=pd.DataFrame()  #To store the probability values of each model\n",
    "\n",
    "        \n",
    "        train_array=X.values #Take the values from training dataset X as dataframe values\n",
    "        label_array=y #Take the label data y\n",
    "        \n",
    "        feature_list=X.columns.values   #Get the orginal descriptive feature list from the base data. (For TASK 8)\n",
    "        \n",
    "        #The below loop is use split the input data into k-folds(train and test) of the the base estimators. (Cross Validation)\n",
    "        for train_index, test_index in kfold.split(train_array):\n",
    "            \n",
    "            X_train, X_test=train_array[train_index], train_array[test_index]\n",
    "            y_train, y_test=label_array[train_index], label_array[test_index]\n",
    "                    \n",
    "        #TASK 3: The following code takes a parameter probability_flag to decide whether the Stack Layer classifier should be trained on label outputs of Base Estimators or probability outputs of Base Estimators\n",
    "            counter=1\n",
    "            if self.probability_flag==False:\n",
    "                pred_array=pd.DataFrame()\n",
    "                for lists in self.modelslists:\n",
    "                    lists.fit(X_train,y_train)\n",
    "                    modelname='Model' + str(counter)\n",
    "                    counter+=1\n",
    "                    pred_array[modelname]=pd.Series(lists.predict(X_test))  #the dataframe to store prediction data from each model\n",
    "                    \n",
    "            else:\n",
    "                proba_dataframe=[]\n",
    "                for lists in self.modelslists:\n",
    "                    lists.fit(X_train,y_train)\n",
    "                    proba_values=pd.DataFrame(lists.predict_proba(X_test), columns= ['Label' +str(i) for i in range(counter,counter+10)]) #dataframe to store probability predictions data from each model\n",
    "                    counter+=10\n",
    "                    proba_dataframe.append(proba_values)\n",
    "                pred_array=pd.concat(proba_dataframe, axis=1)\n",
    "            \n",
    "           \n",
    "        #TASK 8 : To check if original features flag is true to add original Descriptive Features to the input at stack layer\n",
    "            if self.original_features == True:    \n",
    "                extra_feature_pred_dataframe=pd.DataFrame(X_test,columns=feature_list) #create a new dataframe to add the original descriptive feature list.\n",
    "                pred_array=pd.concat([pred_array,extra_feature_pred_dataframe],axis=1) #now concat the new original list dataframe and the stacked layer dataframe\n",
    "            \n",
    "            pred_array_final=pd.concat([pred_array_final,pred_array]) # Stack Layer Data frame formed\n",
    "            pred_label_final=np.append(pred_label_final,y_test)       # Stack Layer target labels\n",
    "            \n",
    "        #To use whole data to train all the base estimators.         \n",
    "        for i in range(len(self.modelslists)):\n",
    "            (self.modelslists)[i].fit(X,y)\n",
    "        \n",
    "        #TASK 4: STACK LAYER CODE : To specifiy the types of estimators to be used at Stack Layer\n",
    "        if self.aggregator == 'DecTreeClf':\n",
    "            self.aggregator=tree.DecisionTreeClassifier()\n",
    "        elif self.aggregator == 'LogClf':\n",
    "            self.aggregator=linear_model.LogisticRegression()\n",
    "        elif self.aggregator== 'KnnClf':\n",
    "            self.aggregator=KNeighborsClassifier()\n",
    "        else:\n",
    "            self.aggregator=tree.DecisionTreeClassifier()\n",
    "        \n",
    "        (self.aggregator).fit(pred_array_final,pred_label_final)  #Training the stack layer by calling fit function\n",
    "              \n",
    "        # Return the classifier\n",
    "\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        #Declare the DataFrames that are required for this function\n",
    "        pred_array=pd.DataFrame()  #To store the data from the predicted labels from the base estimators\n",
    "        pred_array_final=pd.DataFrame() #To store the final predicted data at Stack Layer\n",
    "        proba_dataframe=pd.DataFrame()  #To store the probability values of each model\n",
    "\n",
    "        #Read the original input features (TASK 8 requirement)\n",
    "        feature_list=X.columns.values   #get the orginal descriptive feature list from the base data. \n",
    "        \n",
    "        #TASK 3: The following code takes a parameter probability_flag to decide whether the Stack Layer classifier should be trained on label outputs of Base Estimators or probability outputs of Base Estimators\n",
    "        counter=1\n",
    "        if self.probability_flag==False:\n",
    "            for lists in self.modelslists:\n",
    "                modelname='Model' + str(counter)\n",
    "                counter+=1\n",
    "                pred_array[modelname]=pd.Series(lists.predict(X))\n",
    "        \n",
    "        else:\n",
    "            proba_dataframe=[]\n",
    "            for lists in self.modelslists:\n",
    "                proba_values=pd.DataFrame(lists.predict_proba(X), columns= ['label' +str(i) for i in range(counter,counter+10)]) #dataframe to store probability predictions data from each model\n",
    "                counter+=10\n",
    "                proba_dataframe.append(proba_values)\n",
    "            pred_array=pd.concat(proba_dataframe, axis=1)\n",
    "            \n",
    "        #TASK 8 : To check if original features flag is true to add original Descriptive Features to the input at stack layer\n",
    "        if self.original_features == True:    \n",
    "            extra_feature_pred_dataframe=pd.DataFrame(X,columns=feature_list) #create a new dataframe to add the original descriptive feature list.\n",
    "            extra_feature_pred_dataframe=extra_feature_pred_dataframe.reset_index(drop=True)\n",
    "            pred_array=pd.concat([pred_array,extra_feature_pred_dataframe],axis=1) #now concat the new original list dataframe and the stacked layer dataframe    \n",
    "        \n",
    "        pred_array_final=pd.concat([pred_array_final,pred_array],ignore_index=True) # Stack Layer Data frame formed\n",
    "        \n",
    "        #To predict the data got from the trained stack layer. It will store predicted labels.\n",
    "        Observation_labels=self.aggregator.predict(pred_array_final)\n",
    "        \n",
    "        #Return the Observation labels i.e the predicted labels\n",
    "        return Observation_labels\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"  Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"    \n",
    "        #Declare the DataFrames that are required for this function\n",
    "        pred_array=pd.DataFrame()  #To store the data from the predicted labels from the base estimators\n",
    "        pred_array_final=pd.DataFrame() #To store the final predicted data at Stack Layer\n",
    "        proba_dataframe=pd.DataFrame()  #To store the probability values of each model\n",
    "        \n",
    "        #Read the original input features (TASK 8 requirement)\n",
    "        feature_list=X.columns.values   #get the orginal descriptive feature list from the base data. \n",
    "        \n",
    "        #TASK 3: The following code takes a parameter probability_flag to decide whether the Stack Layer classifier should be trained on label outputs of Base Estimators or probability outputs of Base Estimators\n",
    "        counter=1\n",
    "        if self.probability_flag==False:\n",
    "            for lists in self.modelslists:\n",
    "                modelname='Model' + str(counter)\n",
    "                counter+=1\n",
    "                pred_array[modelname]=pd.Series(lists.predict(X))\n",
    "        else:\n",
    "            proba_dataframe=[]\n",
    "            for lists in self.modelslists:\n",
    "                proba_values=pd.DataFrame(lists.predict_proba(X), columns= ['label' +str(i) for i in range(counter,counter+10)]) #dataframe to store probability predictions data from each model\n",
    "                counter+=10\n",
    "                proba_dataframe.append(proba_values)\n",
    "            pred_array=pd.concat(proba_dataframe, axis=1)\n",
    "        \n",
    "        #TASK 8 : To check if original features flag is true to add original Descriptive Features to the input at stack layer\n",
    "        if self.original_features == True:    \n",
    "            extra_feature_pred_dataframe=pd.DataFrame(X,columns=feature_list) #create a new dataframe to add the original descriptive feature list.\n",
    "            extra_feature_pred_dataframe=extra_feature_pred_dataframe.reset_index(drop=True)\n",
    "            pred_array=pd.concat([pred_array,extra_feature_pred_dataframe],axis=1) #now concat the new original list dataframe and the stacked layer dataframe    \n",
    "        \n",
    "        pred_array_final=pd.concat([pred_array_final,pred_array])  # Stack Layer Data frame formed\n",
    "        \n",
    "        #To predict the data got from the trained stack layer. It will store predicted probability labels.\n",
    "        Observation_labels=(self.aggregator).predict_proba(pred_array_final) \n",
    "        \n",
    "        #Return the Observation labels i.e the predicted probability labels\n",
    "        return Observation_labels \n",
    "        \n",
    "    #TASK 9 : To find the diversity and predictive power of the base estimators we are using for the Super Learner Classifier\n",
    "    def getBaseClassifierPerformance(self,X,y):\n",
    "        \n",
    "        #To find DIVERSITY between the estimators using Spearman's Correlation Matrix\n",
    "        count = 1\n",
    "        correlation_df = pd.DataFrame() #Dataframe to store the correlation matrix\n",
    "        \n",
    "        #Loop to iterate over all the models and find the correlation on the predicted results of the classifier\n",
    "        for my_model in self.modelslists:\n",
    "            correlation_df['Model' + str(count)] = my_model.predict(X)\n",
    "            count = count + 1\n",
    "            correlation_matrix = correlation_df.corr(method='spearman') #using spearman correlation can help you getting proper scores to check diversity in the predictions of the models.\n",
    "\n",
    "        #Plot the correlation matrix using seaborns heat maps\n",
    "        print('Correlation Matrix for Base Models')\n",
    "        sns.heatmap(correlation_matrix, \n",
    "                    xticklabels=correlation_matrix.columns.values,\n",
    "                    yticklabels=correlation_matrix.columns.values)\n",
    "\n",
    "\n",
    "        # Evaluate each model in turn\n",
    "        accuracy_results = []\n",
    "        labels = ['Model1','Model2','Model3','Model4','Model5','Model6']\n",
    "        scoring = 'accuracy'\n",
    "        for my_model in self.modelslists:\n",
    "            cv_results = cross_val_score(my_model, X, y, cv=cv_folds, scoring=scoring)\n",
    "            accuracy_results.append(cv_results)\n",
    "            \n",
    "        \n",
    "        # boxplot algorithm comparison\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        fig.suptitle('Classifiers Comparison')\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.boxplot(accuracy_results)\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.show()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple test using the SuperLearnClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93333333  0.93333333  1.          1.          0.93333333  0.86666667\n",
      "  0.86666667  1.          1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "clf = SuperLearnerClassifier()\n",
    "iris = load_iris()\n",
    "\n",
    "irisdata=pd.DataFrame(np.c_[iris.data])\n",
    "clf.fit(irisdata, iris.target)\n",
    "scores=cross_val_score(clf, irisdata, iris.target, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the number of folds for all grid searches (should be 5 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41437</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50503</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>131</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18139</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9691</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>...</td>\n",
       "      <td>198</td>\n",
       "      <td>175</td>\n",
       "      <td>207</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25928</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "41437      7       0       0       0       0       0       0       0       0   \n",
       "50503      2       0       0       0       0       0       0       0       0   \n",
       "18139      9       0       0       0       0       0       0       0       0   \n",
       "9691       0       0       0       0       0       0       1       0       0   \n",
       "25928      7       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "41437       0    ...            0         0         0         0         0   \n",
       "50503       0    ...            0         0         0         0        64   \n",
       "18139       0    ...            0         0         0         0         0   \n",
       "9691       82    ...          198       175       207       200         0   \n",
       "25928       0    ...            0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "41437         0         0         0         0         0  \n",
       "50503       131        49         0         0         0  \n",
       "18139         0         0         0         0         0  \n",
       "9691          0         1         0         0         0  \n",
       "25928         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv('fashion-mnist_train.csv')\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data pre-processing and manipulation as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape:  (6000, 784)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = dataset[dataset.columns[1:]]\n",
    "Y = np.array(dataset[\"label\"])\n",
    "\n",
    "X = X/255\n",
    "print('Initial Shape: ', X.shape)\n",
    "\n",
    "\"\"\"\n",
    "Here we will split the data into 3-way hold out dataset. It will contain Training data and Test Data (split by 0.7).\n",
    "The Training data will be further divided into trainingplus valid and validation data. This follows the 3-way hold out strategy.\n",
    "This data will be use for training the classifier and evaluating it with different approaches.\n",
    "\n",
    "\"\"\"\n",
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.55/0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Super Learner Classifier using the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TASK 6: \n",
    "\n",
    "#Define some 10 set of classifiers used as base estimators for Super Learner Classifier\n",
    "DecTreeClf= tree.DecisionTreeClassifier()\n",
    "SVMClf= SVC(probability=True)\n",
    "KnnClf=KNeighborsClassifier(5)\n",
    "SGDClf=linear_model.SGDClassifier(loss='log')\n",
    "LogClf=linear_model.LogisticRegression()\n",
    "NBClf=naive_bayes.GaussianNB()\n",
    "RFClf=RandomForestClassifier()\n",
    "ADBClf=AdaBoostClassifier()\n",
    "BagClf=BaggingClassifier()\n",
    "MLPClf=MLPClassifier()\n",
    "\n",
    "#Create some lists of basemodels that can be passed to Super Learner Classifier\n",
    "basemodellist1=[DecTreeClf,RFClf,KnnClf,NBClf,BagClf,LogClf]\n",
    "basemodellist2=[DecTreeClf,SGDClf,SVMClf,KnnClf,NBClf,ADBClf]\n",
    "basemodellist3=[SGDClf,LogClf,MLPClf,ADBClf,BagClf,KnnClf]\n",
    "\n",
    "Vfold=5 #Specify the Vfolds which needs to be given to the superlearner classifier's base estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearnerClassifier(Vfold=None,\n",
       "            aggregator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "            modelslists=[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_stat...lty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)],\n",
       "            original_features=False, probability_flag=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Super Learner Classifier\n",
    "\n",
    "superlearnerClf=SuperLearnerClassifier(basemodellist1,DecTreeClf,Vfold,probability_flag=False)\n",
    "\n",
    "superlearnerClf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained classifier with predict funtion of superlearner classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.806666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.89      0.81        89\n",
      "          1       0.95      0.95      0.95        88\n",
      "          2       0.64      0.68      0.66        79\n",
      "          3       0.82      0.82      0.82        98\n",
      "          4       0.64      0.67      0.65        96\n",
      "          5       0.80      0.88      0.84        88\n",
      "          6       0.66      0.44      0.53        84\n",
      "          7       0.91      0.88      0.89        88\n",
      "          8       0.92      0.93      0.92        86\n",
      "          9       0.94      0.90      0.92       104\n",
      "\n",
      "avg / total       0.81      0.81      0.80       900\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>106</td>\n",
       "      <td>88</td>\n",
       "      <td>85</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>56</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>100</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  All\n",
       "True                                                            \n",
       "0           79    0    4    2    0    0    4    0    0    0   89\n",
       "1            0   84    0    3    0    0    0    0    1    0   88\n",
       "2            1    1   54    1   18    0    3    0    1    0   79\n",
       "3            5    2    2   80    5    0    3    0    1    0   98\n",
       "4            0    1   18    3   64    1    9    0    0    0   96\n",
       "5            0    0    0    0    0   77    0    5    1    5   88\n",
       "6           21    0    6    7   11    0   37    0    2    0   84\n",
       "7            0    0    0    0    0    9    0   77    1    1   88\n",
       "8            0    0    1    1    2    2    0    0   80    0   86\n",
       "9            0    0    0    0    0    7    0    3    0   94  104\n",
       "All        106   88   85   97  100   96   56   85   87  100  900"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = superlearnerClf.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(y_valid, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained classifier with predict_proba funtion of superlearner classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.806666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.89      0.81        89\n",
      "          1       0.95      0.95      0.95        88\n",
      "          2       0.64      0.68      0.66        79\n",
      "          3       0.82      0.82      0.82        98\n",
      "          4       0.64      0.67      0.65        96\n",
      "          5       0.80      0.88      0.84        88\n",
      "          6       0.66      0.44      0.53        84\n",
      "          7       0.91      0.88      0.89        88\n",
      "          8       0.92      0.93      0.92        86\n",
      "          9       0.94      0.90      0.92       104\n",
      "\n",
      "avg / total       0.81      0.81      0.80       900\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>106</td>\n",
       "      <td>88</td>\n",
       "      <td>85</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>96</td>\n",
       "      <td>56</td>\n",
       "      <td>85</td>\n",
       "      <td>87</td>\n",
       "      <td>100</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0   1   2   3    4   5   6   7   8    9  All\n",
       "True                                                     \n",
       "0           79   0   4   2    0   0   4   0   0    0   89\n",
       "1            0  84   0   3    0   0   0   0   1    0   88\n",
       "2            1   1  54   1   18   0   3   0   1    0   79\n",
       "3            5   2   2  80    5   0   3   0   1    0   98\n",
       "4            0   1  18   3   64   1   9   0   0    0   96\n",
       "5            0   0   0   0    0  77   0   5   1    5   88\n",
       "6           21   0   6   7   11   0  37   0   2    0   84\n",
       "7            0   0   0   0    0   9   0  77   1    1   88\n",
       "8            0   0   1   1    2   2   0   0  80    0   86\n",
       "9            0   0   0   0    0   7   0   3   0   94  104\n",
       "All        106  88  85  97  100  96  56  85  87  100  900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To find the accuracy of probability scores is not straight forward. So here we will consider argument with maximum \n",
    "probability score so we use argmax(axis=1). However this will give similar result as that of predict function because the \n",
    "labels with max probability are only the labels returned by predict function. The below code just shows the way to find accuracy\n",
    "if you are asked to used predict_proba function of classifier\n",
    "\"\"\"\n",
    "# Make a set of predictions for the training data\n",
    "y_pred = superlearnerClf.predict_proba(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred.argmax(axis=1)) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_valid, y_pred.argmax(axis=1)))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_valid), y_pred.argmax(axis=1), rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Experiment (Task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79529412  0.78588235  0.82783019  0.81279621  0.8047619   0.8\n",
      "  0.82494005  0.80288462  0.83173077  0.80963855]\n"
     ]
    }
   ],
   "source": [
    "#To get the cross validation scores of the trained Super Learner Classifier. \n",
    "#The scoring mechanism is based on Accuracy scores.\n",
    "\n",
    "scores=cross_val_score(superlearnerClf,X_train_plus_valid,y_train_plus_valid,cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Different Stack Layer Approaches (Task 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DESCRIPTION**\n",
    "    If we consider Decision Tree Classifier and Logistic Regression Classifier to train the data at stack layer, then we\n",
    "    will get following four combinations:\n",
    "        1.Label based and Decision tree\n",
    "        2.Label based and Logistic regression\n",
    "        3.Probability based and Decision tree\n",
    "        4.Probability based and Logistic regression.\n",
    "\n",
    "**EXPERIMENTAL METHOD**\n",
    "    -We will use the one-way hold out strategy dataset from the training testing split data.\n",
    "\n",
    "**PERFORMANCE MEASURE**\n",
    "    -We will use F1-measure as a performance measure to get the F1-scores for each different approach and use them for comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_measure: 0.807165226196\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.83      0.76        89\n",
      "          1       0.94      0.97      0.96        88\n",
      "          2       0.64      0.68      0.66        79\n",
      "          3       0.87      0.76      0.81        98\n",
      "          4       0.73      0.75      0.74        96\n",
      "          5       0.89      0.90      0.89        88\n",
      "          6       0.58      0.48      0.52        84\n",
      "          7       0.88      0.84      0.86        88\n",
      "          8       0.91      0.93      0.92        86\n",
      "          9       0.91      0.92      0.91       104\n",
      "\n",
      "avg / total       0.81      0.81      0.81       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Approach 1 : label based and decision tree classifier at stack layer\n",
    "\n",
    "superlearnerClf_1=SuperLearnerClassifier(basemodellist1,DecTreeClf,probability_flag=False)\n",
    "\n",
    "superlearnerClf_1.fit(X_train,y_train)\n",
    "\n",
    "# Perform its evaluation\n",
    "y_pred = superlearnerClf_1.predict(X_valid)\n",
    "\n",
    "#Calculate the weighted F1-Score\n",
    "F1_1 = metrics.f1_score(y_valid, y_pred,average='weighted') \n",
    "print(\"F1_measure: \" +  str(F1_1))\n",
    "\n",
    "print(metrics.classification_report(y_valid, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_measure: 0.81773557683\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.84      0.80        89\n",
      "          1       0.97      0.97      0.97        88\n",
      "          2       0.66      0.71      0.68        79\n",
      "          3       0.84      0.79      0.81        98\n",
      "          4       0.70      0.69      0.69        96\n",
      "          5       0.91      0.88      0.89        88\n",
      "          6       0.59      0.56      0.58        84\n",
      "          7       0.85      0.93      0.89        88\n",
      "          8       0.94      0.92      0.93        86\n",
      "          9       0.94      0.88      0.91       104\n",
      "\n",
      "avg / total       0.82      0.82      0.82       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 2 : label based and logistic regression classifier at stack layer\n",
    "\n",
    "superlearnerClf_2=SuperLearnerClassifier(basemodellist1,LogClf,probability_flag=False)\n",
    "\n",
    "superlearnerClf_2.fit(X_train,y_train)\n",
    "\n",
    "# Perform its evaluation\n",
    "y_pred = superlearnerClf_2.predict(X_valid)\n",
    "\n",
    "#Calculate the weighted F1-Score\n",
    "F1_2 = metrics.f1_score(y_valid, y_pred,average='weighted') \n",
    "print(\"F1_measure: \" +  str(F1_2))\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_measure: 0.794326877288\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.69      0.66        89\n",
      "          1       0.96      0.97      0.96        88\n",
      "          2       0.71      0.62      0.66        79\n",
      "          3       0.87      0.79      0.82        98\n",
      "          4       0.69      0.69      0.69        96\n",
      "          5       0.93      0.86      0.89        88\n",
      "          6       0.47      0.55      0.51        84\n",
      "          7       0.89      0.90      0.89        88\n",
      "          8       0.92      0.92      0.92        86\n",
      "          9       0.89      0.91      0.90       104\n",
      "\n",
      "avg / total       0.80      0.79      0.79       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 3 : probability label based and decision tree classifier at stack layer\n",
    "\n",
    "superlearnerClf_3=SuperLearnerClassifier(basemodellist1,DecTreeClf,probability_flag=True)\n",
    "\n",
    "superlearnerClf_3.fit(X_train,y_train)\n",
    "\n",
    "# Perform its evaluation\n",
    "y_pred = superlearnerClf_3.predict(X_valid)\n",
    "\n",
    "#Calculate the weighted F1-Score\n",
    "F1_3 = metrics.f1_score(y_valid, y_pred,average='weighted') \n",
    "print(\"F1_measure: \" +  str(F1_3))\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_measure: 0.804962640804\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.80      0.74        89\n",
      "          1       0.93      0.97      0.95        88\n",
      "          2       0.68      0.63      0.66        79\n",
      "          3       0.89      0.81      0.84        98\n",
      "          4       0.67      0.74      0.70        96\n",
      "          5       0.93      0.88      0.90        88\n",
      "          6       0.53      0.48      0.50        84\n",
      "          7       0.89      0.90      0.89        88\n",
      "          8       0.93      0.90      0.91        86\n",
      "          9       0.90      0.92      0.91       104\n",
      "\n",
      "avg / total       0.81      0.81      0.80       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Approach 4 : probability label based and logistic regression classifier at stack layer\n",
    "\n",
    "superlearnerClf_4=SuperLearnerClassifier(basemodellist1,LogClf,probability_flag=False)\n",
    "\n",
    "superlearnerClf_4.fit(X_train,y_train)\n",
    "\n",
    "# Perform its evaluation\n",
    "y_pred = superlearnerClf_4.predict(X_valid)\n",
    "\n",
    "#Calculate the weighted F1-Score\n",
    "F1_4 = metrics.f1_score(y_valid, y_pred,average='weighted') \n",
    "print(\"F1_measure: \" +  str(F1_4))\n",
    "\n",
    "print(metrics.classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have got all the F1-scores of different approaches. We will plot them using bar graph for better understanding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAE8CAYAAAD+N9R0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZJU0P8HwEkfVJMWeuUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c598d84ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the bar labels\n",
    "bar_labels = ['Approach 1', 'Approach 2', 'Approach 3', 'Approach 4']\n",
    "\n",
    "# Create the x position of the bars and give bar graph parameters\n",
    "N=4\n",
    "x_pos = np.arange(N)\n",
    "width=0.2\n",
    "\n",
    "# Create the plot bars\n",
    "#accuracies=(accuracy_1,accuracy_2,accuracy_3,accuracy_4)\n",
    "f1scores=(F1_1,F1_2,F1_3,F1_4)\n",
    "\n",
    "#We can plot other performance factors along with F1-measures, therefore we can use subplots.\n",
    "fig, ax =plt.subplots(figsize=(10,5))\n",
    "\n",
    "# Plot a bar graph. \n",
    "\n",
    "plt.bar([p + width*1.2 for p in x_pos],# using the data from the mean_values\n",
    "        f1scores, \n",
    "        # with a y-error lines set at variance\n",
    "        width ,\n",
    "        # aligned in the center\n",
    "        color='teal'\n",
    "        )\n",
    "\n",
    "\n",
    "# add a grid\n",
    "plt.grid(color='silver')\n",
    "\n",
    "# set axes labels and title\n",
    "ax.set_ylabel('F1-Scores')\n",
    "\n",
    "# Set the chart's title\n",
    "ax.set_title('Approach Comparison Graph')\n",
    "ax.set_xticks([p + width for p in x_pos])\n",
    "ax.set_xticklabels(bar_labels)\n",
    "plt.ylim(0,1)\n",
    "plt.legend(['F1-Measure'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS:\n",
    "The Approach 2 gives the best F1-score as 0.8177. So, we can say the combination of Label based and Logistic Regression Classifier is good at the stack layer for this data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Through SuperLearnerClassifier Architectures & Parameters (Task 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a grid search experiment to detemrine the optimal architecture and hyper-parameter values for the SuperLearnClasssifier for the MNIST Fashion classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the limitation of computation power the Base model lists given to grid search is reduced to set of 3 Base Estimators. So in this way two lists of 3 base-types models is given as an input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'aggregator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              presort=False, random_state=None, splitter='best'),\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  GaussianNB(priors=None)],\n",
       " 'probability_flag': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.80909090909090908"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  31.78589125,   28.90822191,  148.83256917,  148.85872545,\n",
       "          28.45852566,   28.38961043,  146.51124811,  164.28419819]),\n",
       " 'mean_score_time': array([ 0.11634912,  0.12042036,  5.39656086,  5.3459455 ,  0.10867972,\n",
       "         0.11869059,  5.33418112,  6.02206068]),\n",
       " 'mean_test_score': array([ 0.76575758,  0.80393939,  0.75787879,  0.79424242,  0.75666667,\n",
       "         0.80909091,  0.75121212,  0.79363636]),\n",
       " 'mean_train_score': array([ 0.87582815,  0.94386489,  0.84409002,  0.86393951,  0.87628794,\n",
       "         0.94417082,  0.85196868,  0.86106089]),\n",
       " 'param_aggregator': masked_array(data = [ DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best')\n",
       "  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best')\n",
       "  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best')\n",
       "  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best')\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)\n",
       "  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_modelslists': masked_array(data = [ [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False), GaussianNB(priors=None)]\n",
       "  [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False), GaussianNB(priors=None)]\n",
       "  [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False)]\n",
       "  [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False)]\n",
       "  [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False), GaussianNB(priors=None)]\n",
       "  [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False), GaussianNB(priors=None)]\n",
       "  [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False)]\n",
       "  [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "             max_features=None, max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=None, splitter='best'), KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'), SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False)]],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_probability_flag': masked_array(data = [True False True False True False True False],\n",
       "              mask = [False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'aggregator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               presort=False, random_state=None, splitter='best'),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    GaussianNB(priors=None)],\n",
       "   'probability_flag': True},\n",
       "  {'aggregator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               presort=False, random_state=None, splitter='best'),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    GaussianNB(priors=None)],\n",
       "   'probability_flag': False},\n",
       "  {'aggregator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               presort=False, random_state=None, splitter='best'),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "               metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "               weights='uniform'),\n",
       "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "      tol=0.001, verbose=False)],\n",
       "   'probability_flag': True},\n",
       "  {'aggregator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "               max_features=None, max_leaf_nodes=None,\n",
       "               min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "               min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "               presort=False, random_state=None, splitter='best'),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "               metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "               weights='uniform'),\n",
       "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "      tol=0.001, verbose=False)],\n",
       "   'probability_flag': False},\n",
       "  {'aggregator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    GaussianNB(priors=None)],\n",
       "   'probability_flag': True},\n",
       "  {'aggregator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "              intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "              penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "              verbose=0, warm_start=False),\n",
       "    GaussianNB(priors=None)],\n",
       "   'probability_flag': False},\n",
       "  {'aggregator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "               metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "               weights='uniform'),\n",
       "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "      tol=0.001, verbose=False)],\n",
       "   'probability_flag': True},\n",
       "  {'aggregator': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False),\n",
       "   'modelslists': [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                presort=False, random_state=None, splitter='best'),\n",
       "    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "               metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "               weights='uniform'),\n",
       "    SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "      decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "      max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "      tol=0.001, verbose=False)],\n",
       "   'probability_flag': False}),\n",
       " 'rank_test_score': array([5, 2, 6, 3, 7, 1, 8, 4]),\n",
       " 'split0_test_score': array([ 0.7560241 ,  0.8253012 ,  0.74246988,  0.81626506,  0.76054217,\n",
       "         0.81777108,  0.7560241 ,  0.81626506]),\n",
       " 'split0_train_score': array([ 0.86836115,  0.94423369,  0.84522003,  0.87253414,  0.87367223,\n",
       "         0.94954476,  0.85053111,  0.87139605]),\n",
       " 'split1_test_score': array([ 0.75794251,  0.79878971,  0.74281392,  0.79576399,  0.74281392,\n",
       "         0.80181543,  0.75037821,  0.7881997 ]),\n",
       " 'split1_train_score': array([ 0.87002653,  0.93823418,  0.8359227 ,  0.85865858,  0.87608943,\n",
       "         0.93937097,  0.84766957,  0.85676393]),\n",
       " 'split2_test_score': array([ 0.7473525 ,  0.7806354 ,  0.75340393,  0.76399395,  0.73524962,\n",
       "         0.78971256,  0.75037821,  0.76399395]),\n",
       " 'split2_train_score': array([ 0.87533156,  0.95035998,  0.85145889,  0.85373247,  0.87949981,\n",
       "         0.9480864 ,  0.85676393,  0.84501705]),\n",
       " 'split3_test_score': array([ 0.78115502,  0.79787234,  0.75531915,  0.79483283,  0.77963526,\n",
       "         0.80547112,  0.73556231,  0.80395137]),\n",
       " 'split3_train_score': array([ 0.88720666,  0.94852385,  0.83573051,  0.86903861,  0.87963664,\n",
       "         0.95230886,  0.84973505,  0.87244512]),\n",
       " 'split4_test_score': array([ 0.78658537,  0.81707317,  0.79573171,  0.80030488,  0.7652439 ,\n",
       "         0.83079268,  0.76371951,  0.79573171]),\n",
       " 'split4_train_score': array([ 0.87821483,  0.93797277,  0.852118  ,  0.86573374,  0.8725416 ,\n",
       "         0.93154312,  0.85514372,  0.8596823 ]),\n",
       " 'std_fit_time': array([ 1.22281811,  0.16877407,  2.52793564,  1.77860066,  0.19140114,\n",
       "         0.19823383,  1.39928965,  3.00640011]),\n",
       " 'std_score_time': array([ 0.00910257,  0.01599582,  0.0268887 ,  0.04785085,  0.00910977,\n",
       "         0.00782819,  0.0287758 ,  0.09335762]),\n",
       " 'std_test_score': array([ 0.01525302,  0.0157334 ,  0.01958016,  0.01699111,  0.01591078,\n",
       "         0.01404336,  0.00921249,  0.0175226 ]),\n",
       " 'std_train_score': array([ 0.00670564,  0.00510784,  0.00716397,  0.00685798,  0.00291363,\n",
       "         0.00765423,  0.00342384,  0.01014086])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Perform a Grid Search\n",
    "\"\"\"\n",
    "DESCRIPTION\n",
    "\n",
    "Parameters:\n",
    "\n",
    "1. modelslists       : It will take the lists of the base models combinations required for the Super Learner Classifier whose output will be used\n",
    "                       to fill the stack layer.\n",
    "2. aggregator        : This will take the classifier combinations used for training the stack layer.\n",
    "3. probability_flag  : This will take the probability flag True or False to decide whether the label based or probability based\n",
    "                       data needs to be send to stack layer.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "BaseModelsList_1=[DecTreeClf,LogClf,NBClf]\n",
    "BaseModelsList_2=[DecTreeClf,KnnClf,SVMClf]\n",
    "\n",
    "\n",
    "param_grid ={'modelslists': [BaseModelsList_1,BaseModelsList_2], \\\n",
    "             'aggregator': [DecTreeClf,LogClf],\\\n",
    "             'probability_flag': [True,False]  }\n",
    "\n",
    "grid_superlearnerClf=SuperLearnerClassifier()\n",
    "\n",
    "# Perform the grid search\n",
    "my_tuned_superlearner = GridSearchCV(grid_superlearnerClf, \\\n",
    "                                param_grid, cv=cv_folds, verbose = 0, \\\n",
    "                            return_train_score=True)\n",
    "my_tuned_superlearner.fit(X_train, y_train)\n",
    "\n",
    "# Print details of grid searches\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_superlearner.best_params_)\n",
    "display(my_tuned_superlearner.best_score_)\n",
    "display(my_tuned_superlearner.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the model selected by the grid search on a hold-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.784444444444\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.75      0.74       102\n",
      "          1       0.94      0.91      0.92        87\n",
      "          2       0.68      0.69      0.68        97\n",
      "          3       0.72      0.76      0.74        93\n",
      "          4       0.62      0.76      0.68        88\n",
      "          5       0.84      0.87      0.86        85\n",
      "          6       0.65      0.49      0.56        96\n",
      "          7       0.91      0.87      0.89        94\n",
      "          8       0.89      0.88      0.88        74\n",
      "          9       0.94      0.92      0.93        84\n",
      "\n",
      "avg / total       0.79      0.78      0.78       900\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>6.0</th>\n",
       "      <th>7.0</th>\n",
       "      <th>8.0</th>\n",
       "      <th>9.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>105</td>\n",
       "      <td>84</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>108</td>\n",
       "      <td>88</td>\n",
       "      <td>72</td>\n",
       "      <td>90</td>\n",
       "      <td>73</td>\n",
       "      <td>82</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  All\n",
       "True                                                            \n",
       "0           77    1    4   13    1    0    5    0    1    0  102\n",
       "1            2   79    0    6    0    0    0    0    0    0   87\n",
       "2            5    1   67    0   17    0    7    0    0    0   97\n",
       "3            5    1    5   71    8    0    3    0    0    0   93\n",
       "4            0    2    6    4   67    0    7    0    2    0   88\n",
       "5            0    0    1    0    0   74    1    5    1    3   85\n",
       "6           15    0   14    5   12    0   47    0    3    0   96\n",
       "7            0    0    0    0    0    9    0   82    1    2   94\n",
       "8            1    0    2    0    3    0    2    1   65    0   74\n",
       "9            0    0    0    0    0    5    0    2    0   77   84\n",
       "All        105   84   99   99  108   88   72   90   73   82  900"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use the hold out dataset i.e in our case X_test  for the predictions\n",
    "\n",
    "# Make a set of predictions for the test data\n",
    "y_tuned_pred = my_tuned_superlearner.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_tuned_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_valid, y_tuned_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_valid), y_tuned_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Adding Original Descriptive Features at the Stack Layer (Task 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of adding original descriptive features at the stack layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.728888888889\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.67      0.70       102\n",
      "          1       0.96      0.91      0.93        87\n",
      "          2       0.60      0.63      0.61        97\n",
      "          3       0.75      0.62      0.68        93\n",
      "          4       0.59      0.58      0.59        88\n",
      "          5       0.80      0.82      0.81        85\n",
      "          6       0.44      0.53      0.48        96\n",
      "          7       0.90      0.82      0.86        94\n",
      "          8       0.73      0.86      0.79        74\n",
      "          9       0.91      0.92      0.91        84\n",
      "\n",
      "avg / total       0.74      0.73      0.73       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Now that we have got the best parameters combination from the above task (Grid Search). We will use this parameters \n",
    "combination to fit superlearner classifier and evaluate the impact after adding Original Descriptive Features at the Stack Layer\n",
    "on the same data which was used for Grid Search.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "superlearnerClf_new=SuperLearnerClassifier(BaseModelsList_1,DecTreeClf,probability_flag=False,original_features=True)\n",
    "\n",
    "superlearnerClf_new.fit(X_train,y_train)\n",
    "\n",
    "y_pred_new = superlearnerClf_new.predict(X_valid)\n",
    "\n",
    "accuracy_1 = metrics.accuracy_score(y_valid, y_pred_new) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy_1))\n",
    "\n",
    "print(metrics.classification_report(y_valid, y_pred_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVATION OF TASK 8**\n",
    "\n",
    "**The Accuracy score after adding Original Descriptive Features to Stack Layer decreases by some extent. The reason for the decrease can be because of the Curse of Dimentionality. So, as per the Curse of Dimentionality the more features you add there exists some point where your performance starts degrading. The same is happening in above case, adding more input features gave decreased accuracy i.e it degraded the performance than the performance of best tuned super learner classifier we got in the previous result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Ensemble Model (Task 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an analysis to investigate the strength of the base estimators and the strengths of the correlations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Again as declared earlier we can declare the classifier list\n",
    "\n",
    "DecTreeClf= tree.DecisionTreeClassifier()\n",
    "SVMClf= SVC(probability=True)\n",
    "KnnClf=KNeighborsClassifier(5)\n",
    "SGDClf=linear_model.SGDClassifier(loss='log')\n",
    "LogClf=linear_model.LogisticRegression()\n",
    "NBClf=naive_bayes.GaussianNB()\n",
    "RFClf=RandomForestClassifier()\n",
    "ADBClf=AdaBoostClassifier()\n",
    "BagClf=BaggingClassifier()\n",
    "MLPClf=MLPClassifier()\n",
    "\n",
    "#We can use the following base estimators list to investigate the strength of the base estimators and the strengths of the correlations between them.\n",
    "basemodellist1=[DecTreeClf,RFClf,KnnClf,NBClf,BagClf,LogClf]\n",
    "basemodellist2=[DecTreeClf,SGDClf,SVMClf,KnnClf,NBClf,ADBClf]\n",
    "basemodellist3=[SGDClf,LogClf,MLPClf,ADBClf,BagClf,KnnClf]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The base model list should be same as that used during fitting the Super Learner Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix for Base Models\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD7CAYAAABUt054AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c71852ed68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAGPCAYAAABxvvVbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHrtJREFUeJzt3X+U3XV95/HnJKPGmMQZ62QFWq1byDvoalCi/CikqRb2\nLKKiKG2DFYNsQaEi9cCq2+6eXe3KKWD9sUUE6mGrUuqxsv5A03CKGiALCqwKmLyzUVuplTJmM0kU\nDSaZ/eP7ne5lzGTuDJ/7vXPvPB/ncDL3+/P9fedy55XP53vvHRgfH0eSJEllLOh2AZIkSf3EcCVJ\nklSQ4UqSJKkgw5UkSVJBhitJkqSCDFeSJEkFDXa7AEllRMRC4GJgHdX/208GPg/8p8zcGxE3AA9k\n5pUFz/kq4Lcy820RcQzwN8Au4AbgyMx8W6lzTTrvEcCfAMcCB4CfAf8tMz/bifPNRkQcDnw6M0/s\ndi2SmmW4kvrHR4Bh4OWZuSsingZ8Erge+L1OnDAzPwd8rn74KuDLmXleJ841ISJGgM3AHwHrM3M8\nIlYBt0bEo5l5ayfP367M/CfAYCXNQ4YrqQ9ExHOBs4HDMnM3QGb+JCIu4CC/4CPiXOB8qtGtZwCX\nZ+ZHIuJZwF8Cz6w3vSUz//gQy98EvA74K+CtwMKIeCpwK/C6zDw9Ip4OfBB4AfAk4O+ASzNzX0Ts\nBT4LrKrrfyXwGuAxYAfwpsz84aTy3wrckZkfn1iQmd+MiDOBsfr6TgauABbXx/qjzNxQ13sm8FTg\nV4HvA38OXASsAN6fmVfV2/0u1a0TRwA/AM7JzH+KiOOBPwWeAhwG3JqZb46IXwVuB7bUxz6nXrck\nIlYCfwEsAgaA6zPz6oh4EvB+4OXAfuBu4JLM3BMRf081Avhy4NnAX2fmZZP/LiXNPd5zJfWHFwMP\nTgSrCZn5cGZ+pnVZRCwB/j1wWma+CPhtqrBAvfy7mfli4GTgqDocTbV84jyfBK6hCgBnT6rtz4B7\nM/NY4EVUAe0P63VPBj6fmQH8M/B24CWZuRrYCBx3kGtdDdw5eWFm3p6Z90fELwGfBi7OzBdShZxP\n1AGUuv71VGHqXwG/QxVgTgPeGxETr4u/DlyYmc8D7gU+VC+/mGqq9TjgecCrIuLYet0vA+/JzBVA\nayi8tL7OY+vzrKnP80fA4VThchXVa/IVLfstycyTqQLyH7Rcg6Q5zHAl9YcDtPn/c2b+GDgdeEVE\nvAf4j8CSevUG4MyI+CLVyNY7M3PXIZa343Tg/Ij4BlVIeSnVKNaE2+s/fwB8E7gvIq4EvpGZ/3MW\n13ocsD0z766v90GqMLa2Xv/1zHwoMw8A3wM21j9/h2pkaXG93cbM3Fb/fB3wb+ufzwGGIuLdwNX1\n9hP92wf8r4PUdDNwWUR8Bngt8Lb6nP8OuCYzf14//nC9bMJn62v4AfAI1SijpDnOcCX1h68BR0fE\n0taFEXFERNxST9VNLPtl4BvAc4A7qEZPAMjMrwPPBa6lmtr6WkScONXyNmtbCLw+M4/JzGOows9F\nLet/XJ/7APAbwJuopgT/LCI+eJDj3QUcP3lhRJwfEX/IwV/XFlBNSQLsnbTu51PUvW/S/vvrn2+n\nGn3aCvxX4B+ppvoA9mZm634AZOYXgKOAT1GN3t0fEb92kFpb6wT4acvP4y3nkTSHGa6kPlCPbHwS\n+FhELAOo/7wa2JGZrb+kVwOjwHsz82+pRpaIiIURcTnwx/WI0cXAg8CKqZa3Wd7fApdExEBEPIXq\nBviLJm9U35T+ALAlM99HNZ246iDH+yiwNiLOjoiBet9jqYLO/VThKyLipfW65wNrgK+0We+El9fv\nSgS4APh8RAxT9e8/1NOtRwBHUgXIKUXEjcBvZ+ZNVPeM7QZ+hao3F0TEk+ppwgup7leT1MMMV1L/\neCvwbWBzPQV3d/148rv3NlKNtmRE/G+qm6VHqULCB4BjIuIB4B6qabO/OsTydrwNeBpV8PlW/eef\nTt4oM79JNbJzT0TcA5wLXHKQ7f4v1RTfmcADEXE/1U3pb87MWzPzR8DrgQ/X626kelfhtsnHmsY/\nAh+PiIkb1N+emTuB91FNXd4DvItqyvHIaY71HuDsiPgm1d/LzcBXgfcCD1ONJG6hGrW6eIZ1Sppj\nBsbHx7tdgyTNKRPvgszM07tdi6Te48iVJElSQY5cSZIkFeTIlSRJUkGGK0mSpIIMV5IkSQUZriRJ\nkgoyXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKkgw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlxJkiQV\nZLiSJEkqyHAlSZJUkOFKkiSpIMOVJElSQYYrSZKkggxXkiRJBRmuJEmSCjJcSZIkFWS4kiRJKshw\nJUmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmSJBVkuJIkSSpocLoNImIBcDWw\nCtgLnJeZ21vWnw28A9gPfCwzP1Ivvw/YXW/2vcxcX7h2SZKkOWfacAWcASzKzBMi4njgKuDVLeuv\nBJ4P/Bj4dkTcBPwUGMjMtYXrlSRJmtPaCVcnARsAMvOuiFg9af23gKcD+4ABYJxqlGtxRGysz/Hu\nzLzrUCcZHd0zPsPa54Th4cXs3Plot8uYV+x58+x58+x58+x583q55yMjSwemWtdOuFoG7Gp5vD8i\nBjNzX/34AeBe4CfAZzJzLCIepRrRuh44CvhSRETLPr9geHgxg4ML2yhn7hkZWdrtEuYde948e948\ne948e968fux5O+FqN9B65QsmQlJEvBB4BfBcqmnBT0TE64HPAdszcxzYFhE7gMOAh6Y6SQ8nV0ZH\n93S7jHnFnjfPnjfPnjfPnjevl3t+qFDYzrsF7wROA6jvubq/Zd0uqvurfpqZ+4FHgGHgXKp7s4iI\nw6lGv344i9olSZJ6SjsjVzcDp0TEZqp7qtZHxDpgSWZeGxEfBe6IiMeA7wA31PvdEBF3UN2Dde6h\npgQlSZL6xbThKjMPABdMWry1Zf01wDUH2XXdEytNkiSp9/ghopIkSQUZriRJkgoyXEmSJBVkuJIk\nSSrIcCVJklSQ4UqSJKkgw5UkSVJBhitJkqSC2vmEdkmSpLasWXMcW7du6cixV648mk2b7u7IsUsy\nXEmSpGJmEn6WL1/GI4/s7mA13WG4kiRJU1qx4tmMjY117PjLly/r2LGHhobYtu37HTv+VAxXkiRp\nSmNjYx0bXRoZWcro6J6OHBs6G9wOxRvaJUmSCjJcSZIkFWS4kiRJKshwJUmSVJDhSpIkqSDDlSRJ\nUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmSJBXk199IkqQpnXrFWVx422XdLmNWTr3irK6c13AlSZKm\ntPHST/X2dwuec33Hjj8VpwUlSZIKMlxJkiQV5LSgJEk6pOXLl3W7hFkZGhrqynkNV5IkaUqdut8K\nqtDWyeN3i9OCkiRJBRmuJEmSCjJcSZIkFWS4kiRJKshwJUmSVJDvFpQk9a01a45j69YtHTn2ypVH\ns2nT3R05tnqb4Uqap/ylo160YsWzGRsb63YZAGzdumVGn/80NDTEtm3f72BFmisMV9I8NZPw06+f\nRaPeMzY21tvfczcPzPQfbjPpS6/8w81wJUmSiplJ+Ol0oO0Wb2iXJEkqyHAlSZJU0LTTghGxALga\nWAXsBc7LzO0t688G3gHsBz6WmR+Zbh9J5XX6Rt9O3i/ijb6S+kk791ydASzKzBMi4njgKuDVLeuv\nBJ4P/Bj4dkTcBPzmNPuoz53z8d9nyRHd+TbyJ+rHPxjjf/zetd0uY8a80VeS5oZ2wtVJwAaAzLwr\nIlZPWv8t4OnAPmAAGG9jH/W5L73jpt7+Rd+D4UqSNDe0E66WAbtaHu+PiMHM3Fc/fgC4F/gJ8JnM\nHIuI6fb5BcPDixkcXDjD8ueGkZGl3S5hTurV0Yjh4eGe/TvtZN2d7kmv9ryT7MnB+TzvL/3Yk3bC\n1W6g9coXTISkiHgh8ArguVTTgp+IiNcfap+p7Nz56EzqnjP69W2kT1QnPxOpiZ736t9pp+q2583z\ntWVqPs/7Ry8/zw8VCtt5t+CdwGkA9f1T97es2wX8FPhpZu4HHgGGp9lHkiSpb7UzcnUzcEpEbKa6\np2p9RKwDlmTmtRHxUeCOiHgM+A5wA9X9V4/bpyPVS/oXp15xFhfedlm3y5iVU684q9slSFIxA+Pj\n492uAYDR0T1zo5AZ6uUhzV5lzw+uk19R08SbCPx6ncfzeX5wPs/7Sy8/z0dGlg5Mtc4PEZUkSSrI\ncCVJklSQ4UqSJKkgw5UkSVJBhitJkqSC2vkoBkmS5gQ/ckS9wHAl9ZFe/cqhoaHe/JJvNW/jpZ/q\ndgmzNjQ0BOd0uwo1wXAl9Yle/8ohqR2dfJ77OVQqxXuuJEmSCjJcSZIkFWS4kiRJKshwJUmSVJA3\ntEuS+taaNcexdeuWtrefyTtuV648mk2b7p5NWepzhitJUt+aSfjxXbEqxWlBSZKkggxXkiRJBRmu\nJEmSCjJcSZIkFWS4kiRJKshwJUmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmS\nJBVkuJIkSSrIcCVJklSQ4UqSJKkgw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlxJkiQVZLiSJEkq\nyHAlSZJUkOFKkiSpIMOVJElSQYYrSZKkggan2yAiFgBXA6uAvcB5mbm9Xvcs4KaWzY8B3pmZ10TE\nfcDuevn3MnN90colSZLmoGnDFXAGsCgzT4iI44GrgFcDZObDwFqAiDgB+BPguohYBAxk5tpOFC1J\nkjRXtTMteBKwASAz7wJWT94gIgaADwNvycz9VKNciyNiY0TcVocySZKkvtfOyNUyYFfL4/0RMZiZ\n+1qWvRJ4MDOzfvwocCVwPXAU8KWIiEn7PM7w8GIGBxfOrPo5YmRkabdLmHfsefPsefPsefPsefP6\nsefthKvdQOuVLzhISHoD8MGWx9uA7Zk5DmyLiB3AYcBDU51k585H26t4jhkZWcro6J5ulzGv2PPm\n2fPm2fPm2fPm9XLPDxUK25kWvBM4DaCe3rv/INusBja3PD6X6t4sIuJwqtGvH7ZXriRJUu9qZ+Tq\nZuCUiNgMDADrI2IdsCQzr42IEWB3PUo14S+AGyLiDmAcOPdQU4KSJEn9YtpwlZkHgAsmLd7asn6U\n6iMYWvd5DFhXokBJkqRe4oeISpIkFWS4kiRJKshwJUmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5Ik\nSQUZriRJkgpq5xPa5501a45j69YtHTn2ypVHs2nT3R05tiRJ6j7D1UHMJPwsX76MRx7Z3cFqJElS\nL5kX4WrFimczNjbWseMvX76sY8ceGhpi27bvd+z4kiSprHkRrsbGxjo2ujQyspTR0T0dOTZ0NrhJ\nkqTyvKFdkiSpIMOVJElSQYYrSZKkggxXkiRJBRmuJEmSCjJcSZIkFWS4kiRJKshwJUmSVJDhSpIk\nqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKmgwW4X0IRT\nrziLC2+7rNtlzMqpV5zV7RIkSdIMzItwtfHST3W7hFkbGhqCc7pdhSRJate8CFePPLK7Y8devnxZ\nR48vSZJ6y7wIVzO1Zs1xbN26pe3tly9f1va2K1cezaZNd8+mLEmS1AMMVwcxk/AzMrKU0dE9HaxG\nkiT1Et8tKEmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQVN+27BiFgAXA2sAvYC52Xm9nrds4Cb\nWjY/BngncO1U+0iSJPWzdkauzgAWZeYJVMHpqokVmflwZq7NzLXAu4D7gOsOtY8kSVI/aydcnQRs\nAMjMu4DVkzeIiAHgw8BbMnN/O/tIkiT1o3Y+RHQZsKvl8f6IGMzMfS3LXgk8mJk5g30eZ3h4MYOD\nC9ute04ZGVna7RLmHXvePHvePHvePHvevH7seTvhajfQeuULDhKS3gB8cIb7PM7OnY+2Ucrc4ye0\nN8+eN8+eN8+eN8+eN6+Xe36oUNjOtOCdwGkAEXE8cP9BtlkNbJ7hPpIkSX2nnZGrm4FTImIzMACs\nj4h1wJLMvDYiRoDdmTl+qH1KFy5JkjQXTRuuMvMAcMGkxVtb1o9SfQTDdPtIkiT1PT9EVJIkqSDD\nlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKkgw5UkSVJBhitJ\nkqSCDFeSJEkFGa4kSZIKMlxJkiQVZLiSJEkqyHAlSZJUkOFKkiSpIMOVJElSQYYrSZKkggxXkiRJ\nBRmuJEmSCjJcSZIkFWS4kiRJKshwJUmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoy\nXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKkgw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlxJkiQVZLiS\nJEkqaHC6DSJiAXA1sArYC5yXmdtb1r8EeD8wADwMvCEzfxYR9wG7682+l5nrSxcvSZI010wbroAz\ngEWZeUJEHA9cBbwaICIGgOuA12Xm9og4D3hORPwDMJCZaztUtyRJ0pzUzrTgScAGgMy8C1jdsm4F\nsAO4JCK+CjwjM5NqlGtxRGyMiNvqUCZJktT32hm5Wgbsanm8PyIGM3Mf8EzgROAiYDvwhYi4BxgF\nrgSuB44CvhQRUe9zUMPDixkcXDjLy+iukZGl3S5h3rHnzbPnzbPnzbPnzevHnrcTrnYDrVe+oCUk\n7QC2Z+YWgIjYQDWy9cF6+TiwLSJ2AIcBD011kp07H51F+d03MrKU0dE93S5jXrHnzbPnzbPnzbPn\nzevlnh8qFLYzLXgncBpAPb13f8u67wJLIuLI+vHJwIPAuVT3ZhERh1ONfv1wpoVLkiT1mnZGrm4G\nTomIzVTvCFwfEeuAJZl5bUS8Gbixvrl9c2beEhFPBm6IiDuAceDcQ00JSpIk9Ytpw1VmHgAumLR4\na8v624CXTtrnMWBdiQIlSZJ6iR8iKkmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoy\nXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKkgw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlxJkiQVZLiS\nJEkqyHAlSZJUkOFKkiSpIMOVJElSQYYrSZKkggxXkiRJBRmuJEmSCjJcSZIkFWS4kiRJKshwJUmS\nVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKkg\nw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlxJkiQVNDjdBhGxALgaWAXsBc7LzO0t618CvB8YAB4G\n3gA8dqh9JEmS+lU7I1dnAIsy8wTgncBVEysiYgC4DlifmScBG4DnHGofSZKkftZOuJoITWTmXcDq\nlnUrgB3AJRHxVeAZmZnT7CNJktS3pp0WBJYBu1oe74+IwczcBzwTOBG4CNgOfCEi7plmn4MaHl7M\n4ODCGV/AXDAysrTbJcw79rx59rx59rx59rx5/djzdsLVbqD1yhe0hKQdwPbM3AIQERuoRqkOtc9B\n7dz5aNtFzyUjI0sZHd3T7TLmFXvePHvePHvePHvevF7u+aFCYTvTgncCpwFExPHA/S3rvgssiYgj\n68cnAw9Os48kSVLfamfk6mbglIjYTPWOwPURsQ5YkpnXRsSbgRvrm9s3Z+Yt9TsMH7dPpy5AkiRp\nLpk2XGXmAeCCSYu3tqy/DXhpG/tIkiT1PT9EVJIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoy\nXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKkgw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlxJkiQVZLiS\nJEkqyHAlSZJUkOFKkiSpIMOVJElSQYYrSZKkggxXkiRJBRmuJEmSCjJcSZIkFWS4kiRJKshwJUmS\nVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKkg\nw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlxJkiQVZLiSJEkqaHC6DSJiAXA1sArYC5yXmdtb1l8C\nnAeM1ovOz8yMiPuA3fWy72Xm+qKVS5IkzUHThivgDGBRZp4QEccDVwGvbll/LPDGzLx3YkFELAIG\nMnNtyWIlSZLmunamBU8CNgBk5l3A6knrjwXeFRF3RMS76mWrgMURsTEibqtDmSRJUt9rZ+RqGbCr\n5fH+iBjMzH3145uAP6eaArw5Ik4H/gG4ErgeOAr4UkREyz6/YHh4MYODC2dzDV03MrK02yXMO/a8\nefa8efa8efa8ef3Y83bC1W6g9coXTISkiBgAPpCZu+rHtwAvAm4FtmfmOLAtInYAhwEPTXWSnTsf\nnd0VdNnIyFJGR/d0u4x5xZ43z543z543z543r5d7fqhQ2M604J3AaQD19N79LeuWAQ9ExJI6aL0M\nuBc4l+reLCLi8Hq7H86meEmSpF7SzsjVzcApEbEZGADWR8Q6YElmXhsR7wa+TPVOwr/LzC9GxJOB\nGyLiDmAcOPdQU4KSJEn9YmB8fLzbNQAwOrpnbhQyQ708pNmr7Hnz7Hnz7Hnz7HnzernnIyNLB6Za\n54eISpIkFWS4kiRJKshwJUmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJkgoyXEmSJBXU\nztffSJIOYsWKZzM2NtbtMmZlaGiIbdu+3+0ypL5kuJKkWRobG+ORR3Z35Nid/lqQ5cuXdezY0nxn\nuJKkWTr1irO48LbLul3GrJx6xVndLkHqW4YrSZqljZd+qrdHrs65vmPHl+Yzb2iXJEkqyHAlSZJU\nkOFKkiSpIMOVJElSQYYrSZKkgny3oCQ9Ab36eVFDQ0PdLkHqW4YrSZqlTn0MA1ShrZPHl9Q5TgtK\nkiQVZLiSJEkqyHAlSZJUkPdcSVJD1qw5jq1bt7S9/Uxull+58mg2bbp7NmVJKsxwJUkNmUn46fR3\nC0rqHKcFJUmSCjJcSZIkFWS4kiRJKshwJUmSVJDhSpIkqSDDlSRJUkGGK0mSpIIMV5IkSQUZriRJ\nkgoyXEmSJBVkuJIkSSrIcCVJklSQ4UqSJKmggfHx8W7XIEmS1DccuZIkSSrIcCVJklSQ4UqSJKkg\nw5UkSVJBhitJkqSCDFeSJEkFGa4kSZIKMlwBEbE2IsYj4ncmLf9WRNzQxv6LIuLvpzn+TS2PF0fE\nnRGx8gmU3dOa7HlE/G5E3F33/JqImJfP+4Z7fmZEfD0ivhYRFz/B0ntW068t9bJrI+LyWZbc8xp+\nnl8SEQ9GxFfq/+IJlt+TGu75SyLi9oi4IyI+HRGLnmD5HTEvf8lMYSvwL0+MiHgB8LTSJ4mI1cAm\n4NdKH7sHdbznEfFU4L3Ab2bmrwNPB04veY4e00TPFwKXA78FnAC8NSKeWfIcPaaR15b62OcDL+jE\nsXtMUz0/FnhjZq6t/8sOnKNXNPHaMgBcB6zPzJOADcBzSp6jlMFuFzCHfBOIiHh6Zu4C3gB8Enh2\nRJwNvB3YC/wf4PeBp9Trh4HtEwepn1AfAgaAHcC5k87zFOA1wMc7ejW9oYme7wVOzMxH68eDwM86\neVFzXMd7npn7I+LozNwXEcuBhcBjTVzcHNXIa0tEnAgcB3wUmLej4rWmXs+PBd4VEc8CbsnM93X0\nqua2Jnq+ol52SUT8G6qez8lA68jV4/0N8No6Hb8U2Az8EvBfgJfVSXkMOB+4AHggM9dQvZhNuA64\nMDPXAl8ELms9QWbemZkPdfpCekhHe56ZBzLznwEi4g+AJcCtnb6oOa6J5/m+iHgt1QvuV4CfdPKC\nekBHex4RhwH/Gbio85fSMzr+PAduqvd9GXBSRMznUXHofM+fCZwI/HeqkfGXR8TLOnpFs2S4erwb\nqYY11wC318sWAA9m5p768Sbg+VQJ+msAmXk38PN6/dHA1RHxFarEfUQjlfeujvc8IhZExJXAKcCZ\nmTnfv1Czked5Zn6mXv5k4I2duJAe0umev57qF88XgXcC6yLiTR26ll7R0Z7XAeIDmfmjzHwMuAV4\nUScvqAd0+nm+A9iemVsy8+dU04KrO3Y1T4DhqkVmfpdqjvhtwCfqxePA8yJiYu74N4BtwLep7ich\nIl4EPGniMNRz8FSJ+wuNFN+jGur5R4FFwBkt04PzVqd7HhHLIuKrEfGUzDxANWp1oKMXNcd1uueZ\n+aHMPLZedzlwY2be0MFLmvMaeG1ZBjwQEUvqoPUy4N6OXVAPaKDn3wWWRMSR9eOTgQc7cjFPkOHq\nF/018CuZua1+/COq4fYvR8RdVP86/AhwDfCvI+IO4EKquWSAtwB/WS+/HPhWk8X3qI71PCJeDLyZ\n6ibf2+p39LymgWua6zrW88zcTXUvxaZ6/Tj//4V2PvO1pXmdfJ7vAt4NfJlqlObBzPxi5y9pzutk\nzx+jej2/MSK+DjyUmbc0cE0zNjA+Pt9nSCRJkspx5EqSJKkgw5UkSVJBhitJkqSCDFeSJEkFGa4k\nSZIKMlxJkiQVZLiSJEkq6P8BXz/rlVKl/wgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c7184dc9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuperLearnerClassifier(Vfold=None,\n",
       "            aggregator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "            modelslists=[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_stat...lty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)],\n",
       "            original_features=False, probability_flag=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=SuperLearnerClassifier(basemodellist1)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.getBaseClassifierPerformance(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 9 RESULTS EXPLANATION**\n",
    "\n",
    "** The diversity is calculated using the correlation matrix. The correlation method used is Spearman correlation. The Heat Map is used for visualizing the correlation scores amongst each base models. The high correlation scores means there is less diversity between two models.For example in above Heat Map Model 1 and Model 4 are less correlated(high diversity) than Model 1 and Model 2 high correlated(less diversity)**\n",
    "\n",
    "**\n",
    "The box plot is used to plot the performance measures here accuracy of the base estimators. Here Model 4 i.e Naive Bayes gives the least accuracy wheresas Logistic Regression Classifier gives highest accuracy amongst each other\n",
    "**\n",
    "\n",
    "**\n",
    "Thus, this explains the ensemble concept i.e base estimators being strong predictors but at same time are weakly correlated with each other.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
